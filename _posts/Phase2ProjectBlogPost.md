# A controlled head ache?
## Intro
My journey through Flatiron is separated into 5 phases. Each phase focuses on a specific area of data science that will be needed in the professional world. Phase 2 just ended and this phase was all about statistics. An area that I have dabbled in somewhat. I always say I have never taken a statistics class in my academic time but many of the classes I have taken have had to use statistics so many of the things that were shown to me in this phase was something I have seen. Now with all this information it had to be applied somewhere. Just with the previous phase we had an end of phase project that had me wanting to pull my hair out.
## Background on project
The project that was given to us was to present to a stakeholder (one of our choosing) and solve a problem that could arise for them. The data was house sales and their relevant information of King County in Washington State from the year 2014 to the year 2015. With this we found that our stakeholder should be a local real estate firm and pitch to them a model we created from the data that would accurately predict a price of a home based on its features. This was to show their clients whether it was a seller or a buyer. 
## Project
### Cleaning
The dataset had over twenty one thousand entries of homes with their vast features. The features being date when sold, year when renovated, its standing condition, number of  bathrooms, bedrooms, floors, square footage of ; living space, lot, basement, above basement, interior of 15 closest neighbors, and lot of of 15 closest neighbors. It also had features based off of its location. It had its zip code, rating on its view, its latitude coordinate, longitude coordinate and whether it was on the water or not. With these vast amounts of features we deemed it too many to handle and decided to exclude features that we deemed as useless or something that would skew our model so heavily that any results given out would require a supercomputer to break down. As well we found ways to change some columns to be more useful with applying information from other columns. An example of this being done is with the square footage of the basement. We saw this column to be somewhat useless since we already had total square footage. But we decided to change change square footage of the basement column to a yes or no style on whether the house had a basement or not. By looking at the total square footage of the house and the square footage above the basement. If we found these two numbers to be the same we found that house to not have a basement. Now if the numbers were different it meant there was in fact a basement and we just created a column with binary values in that each home that had a basement would get a one and if not would have a 0. With some other cleaning done in came where most of the EDA would come about. Normally you want to create a model with features that are highly correlated to it as well as continuous but after plotting some scatter plots we noticed how many of the columns were categorical and had to be dealt with. We had to “OneHotEncode” all of the categorical columns to created columns for each values represented in each individual column and put in binary values(in reference to yes or no) to show whether that house had that new category or not. After creating all the new columns we then had to concat them all into one dataframe. Finally the cleaning was done.
### Predictive model
Now with the dataframe sorted we could start building our models. We set a baseline model(our “simple model”) to see if adding more features to our model could improve it. For our baseline we chose to just add in the feature that highly correlated to our target, price. Doing a simple correlation chart we found that the feature that was continuous with the highest correlation to be total square footage of the home. After doing an X,y train test split to see how our model performed we found that it wasn’t the best model in that it only accounted for under 40% of our data. After many iterations of different combinations of features we found that adding in the zipcode, square footage, waterfront presence, view level, basement(y/n), condition of the home, amount of floors, bedrooms, bathrooms and the grade of the build of the home. This model had the best score out of all the ones we made. We would think that we have done it, we created a great model. Sadly this was not true. After making our models we have to look at the residuals to see the true performance on how well our model performs to price and we found we had an RSME of over one hundred five thousand dollars. That is quite the number when it comes to pricing homes. We thought how could this be that our model isn’t performing to the level we want. We tried logging price, scaling the square footage of the home but nothing improve. It wasn’t until our instructor told our cohort that it was ok if we had a poor performing model. We scratched our heads a bit until he said that with the data given there was no way to actually achieve a well performing model. With that confusion somewhat settled we moved on.
### Inferential model
With knowing our dataset was one that gave us many limitations we moved on to create our inferential model. In our inferential model we added in the square footage of the home, presence of a waterfront, its view level and its zipcode. With square footage being the only continuous we could draw the conclusion that per every square foot of a home it applies 179 dollars in value to a home on average. With the rest of our categorical variables we saw how on average how much more or less homes with said categorical variable were priced based on our average.
## Final Thoughts?
After tons of data cleaning and model creation we found dishearteningly that the dataset given would not allow a proper model to be built to accurately predict price.
